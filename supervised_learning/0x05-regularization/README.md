# Supervised Learning - Regularization

### 0-l2_reg_cost.py - Contains the function definition l2_reg_cost(cost, lambtha, weights, L, m) which calculates the cost of a neural network with L2 regularization. ** Note ** The parameter lambtha is spelled as such to avoid using or overwriting the Python operator lambda.

### 1-l2_reg_gradient_descent.py - Contains the function definition l2_reg_gradient_descent(Y, weights, cache, alpha, lambtha, L) which updates the weights of a neural network with L2 regularization using gradient descent.

### 2-l2_reg_cost.py - Contains the function definition l2_reg_cost(cost) which calculates the cost of a neural network with L2 regularization.

### 3-l2_reg_create_layer.py - Contains the function definition l2_reg_create_layer(prev, n, activation, lambtha) which creates a TensorFlow layer that includes L2 regularization.

### 4-dropout_forward_prop.py - Contains the function definition dropout_forward_prop(X, weights, L, keep_prob) which conducts forward propagation using dropout.

### 5-dropout_gradient_descent.py - Contains the function definition dropout_gradient_descent(Y, weights, cache, alpha) which updates the weights of a neural network with dropout regularization using gradient descent.

### 6-dropout_create_layer.py - Contains the function definition dropout_create_layer(prev, n, activation, keep_prob) which creates a layer of a neural network using dropout (in TensorFlow).
